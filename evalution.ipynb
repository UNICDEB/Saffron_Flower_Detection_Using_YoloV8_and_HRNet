{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c099a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf96d30",
   "metadata": {},
   "source": [
    "1) Quick evaluation: compute MSE, MAE, and pixel error\n",
    "\n",
    "This assumes you have a final_dataset/images/*.jpg and corresponding final_dataset/labels/*.txt containing class x y w h px py (px,py in crop-normalized coords). Also you have a trained model checkpoint hrnet_pluck.pth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from train_hrnet_pluck import HRNetPluckRegressor, predict_on_crop  # or import model class\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = HRNetPluckRegressor(backbone_name=\"hrnet_w18\", pretrained=False).to(device)\n",
    "ck = torch.load(\"hrnet_pluck.pth\", map_location=device)\n",
    "# If checkpoint saved as dict with \"model\"\n",
    "if \"model\" in ck:\n",
    "    model.load_state_dict(ck[\"model\"])\n",
    "else:\n",
    "    model.load_state_dict(ck)\n",
    "model.eval()\n",
    "\n",
    "img_dir = \"Dataset/images\"\n",
    "lbl_dir = \"Dataset/labels\"\n",
    "img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "\n",
    "mse_sum = 0.0\n",
    "mae_sum = 0.0\n",
    "pixel_errs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for fname in img_files:\n",
    "        img_path = os.path.join(img_dir, fname)\n",
    "        lbl_path = os.path.join(lbl_dir, fname.replace(\".jpg\",\".txt\"))\n",
    "        if not os.path.exists(lbl_path): \n",
    "            continue\n",
    "        # read image and label\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w = img.shape[:2]\n",
    "        line = open(lbl_path).read().strip().split()\n",
    "        gt_px = float(line[5])\n",
    "        gt_py = float(line[6])\n",
    "\n",
    "        # preprocess like training (resize & normalize)\n",
    "        # use the same transform as training (here simple resize -> normalize)\n",
    "        import torchvision.transforms as T\n",
    "        from timm.data import resolve_data_config\n",
    "        import timm\n",
    "        tmp = timm.create_model(\"hrnet_w18\", pretrained=False, num_classes=0)\n",
    "        cfg = resolve_data_config({}, model=tmp)\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((256,256)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=cfg[\"mean\"], std=cfg[\"std\"])\n",
    "        ])\n",
    "        inp = transform(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)).unsqueeze(0).to(device)\n",
    "\n",
    "        pred = model(inp)[0].cpu().numpy()  # normalized px,py\n",
    "        gt = np.array([gt_px, gt_py], dtype=float)\n",
    "\n",
    "        # metrics in normalized space\n",
    "        mse = np.mean((pred - gt)**2)\n",
    "        mae = np.mean(np.abs(pred - gt))\n",
    "        mse_sum += mse\n",
    "        mae_sum += mae\n",
    "\n",
    "        # pixel error (assuming crop size = original crop dims; if you kept crops as varied sizes,\n",
    "        # convert normalized coords back by using actual crop-size used during annotation)\n",
    "        # here we assume crop size used for annotation is actual image shape (h,w)\n",
    "        # if you used resized 256x256 then pixel err = norm_err * 256\n",
    "        px_err = np.linalg.norm((pred - gt) * 256)  # pixel error w.r.t model input size\n",
    "        pixel_errs.append(px_err)\n",
    "\n",
    "n = len(img_files)\n",
    "print(f\"MSE (avg): {mse_sum/n:.6f}, MAE (avg): {mae_sum/n:.6f}\")\n",
    "print(f\"Pixel error — mean: {np.mean(pixel_errs):.3f}, med: {np.median(pixel_errs):.3f}, std: {np.std(pixel_errs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2b97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5201a82b",
   "metadata": {},
   "source": [
    "2) Visualize predictions on crops and original images\n",
    "\n",
    "Useful for qualitative debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "from train_hrnet_pluck import HRNetPluckRegressor  # model class\n",
    "\n",
    "# load detector and regressor\n",
    "yolo = YOLO(\"runs/detect/train/weights/best.pt\")   # path to your trained YOLO\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "hrnet = HRNetPluckRegressor(backbone_name=\"hrnet_w18\", pretrained=False).to(device)\n",
    "ck = torch.load(\"hrnet_pluck.pth\", map_location=device)\n",
    "hrnet.load_state_dict(ck.get(\"model\", ck))\n",
    "hrnet.eval()\n",
    "\n",
    "def predict_and_draw(img_path, out_path=None):\n",
    "    orig = cv2.imread(img_path)\n",
    "    h,w = orig.shape[:2]\n",
    "    res = yolo(img_path)[0]\n",
    "    out_img = orig.copy()\n",
    "    for box, cls in zip(res.boxes.xyxy, res.boxes.cls):\n",
    "        cls = int(cls)\n",
    "        x1,y1,x2,y2 = map(int, box.tolist())\n",
    "        cv2.rectangle(out_img, (x1,y1),(x2,y2),(0,255,0),2)\n",
    "        if cls==1:  # side\n",
    "            crop = orig[y1:y2, x1:x2]\n",
    "            if crop.size==0: continue\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "            # preprocess (resize to 256)\n",
    "            import torchvision.transforms as T\n",
    "            from timm.data import resolve_data_config\n",
    "            import timm\n",
    "            tmp = timm.create_model(\"hrnet_w18\", pretrained=False, num_classes=0)\n",
    "            cfg = resolve_data_config({}, model=tmp)\n",
    "            trans = T.Compose([\n",
    "                T.ToPILImage(), T.Resize((256,256)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=cfg[\"mean\"], std=cfg[\"std\"])\n",
    "            ])\n",
    "            inp = trans(crop_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                px,py = hrnet(inp)[0].cpu().numpy()\n",
    "            px_abs = int(px*(x2-x1)+x1)\n",
    "            py_abs = int(py*(y2-y1)+y1)\n",
    "            cv2.circle(out_img,(px_abs,py_abs),5,(0,0,255),-1)\n",
    "            cv2.putText(out_img, f\"pluck\", (px_abs+5,py_abs-5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255),1)\n",
    "\n",
    "    if out_path:\n",
    "        cv2.imwrite(out_path, out_img)\n",
    "    return out_img\n",
    "\n",
    "# example\n",
    "res_img = predict_and_draw(\"test_images/001.jpg\", out_path=\"debug/001_pred.jpg\")\n",
    "cv2.imshow(\"pred\", res_img); cv2.waitKey(0); cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b725f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b05b4452",
   "metadata": {},
   "source": [
    "3) Integrate YOLO → HRNet in a real-time loop (video / camera)\n",
    "\n",
    "If you want a live pipeline for robotics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "# load models like above: yolo, hrnet\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # or video file\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    results = yolo(frame)[0]\n",
    "    for box, cls in zip(results.boxes.xyxy, results.boxes.cls):\n",
    "        cls = int(cls)\n",
    "        x1,y1,x2,y2 = map(int, box.tolist())\n",
    "        cv2.rectangle(frame,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "        if cls==1:\n",
    "            crop = frame[y1:y2,x1:x2]\n",
    "            # preprocess + predict same as above; draw point\n",
    "            # ...\n",
    "    cv2.imshow(\"live\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27: break\n",
    "cap.release(); cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20340653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbe2b763",
   "metadata": {},
   "source": [
    "4) Export HRNet to TorchScript / ONNX for faster deployment\n",
    "\n",
    "TorchScript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HRNetPluckRegressor(backbone_name=\"hrnet_w18\", pretrained=False)\n",
    "ck = torch.load(\"hrnet_pluck.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(ck.get(\"model\", ck))\n",
    "model.eval().cpu()\n",
    "\n",
    "example = torch.rand(1,3,256,256)\n",
    "traced = torch.jit.trace(model, example)\n",
    "traced.save(\"hrnet_pluck_ts.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6327d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16472556",
   "metadata": {},
   "source": [
    "ONNX (useful for some embedded accelerators):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad968df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.eval().cpu()\n",
    "dummy = torch.randn(1,3,256,256)\n",
    "torch.onnx.export(model, dummy, \"hrnet_pluck.onnx\",\n",
    "                  input_names=[\"input\"], output_names=[\"output\"],\n",
    "                  opset_version=12, dynamic_axes={\"input\":{0:\"batch\"}, \"output\":{0:\"batch\"}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2ef84",
   "metadata": {},
   "source": [
    "5) Improvements & ideas to try (practical list)\n",
    "\n",
    "Heatmap supervision: predict a small Gaussian heatmap and use soft-argmax — often more stable than direct regression.\n",
    "\n",
    "Data augmentations: brightness, contrast, small rotation, scaling, cutout. Ensure keypoint transforms are applied consistently.\n",
    "\n",
    "Loss: mix MSE + MAE, or Huber loss to be robust to outliers.\n",
    "\n",
    "Model ensembling: average predictions from multiple checkpoints.\n",
    "\n",
    "Test-time augmentation (flip & average).\n",
    "\n",
    "Use a small bounding-box margin when cropping (include some context).\n",
    "\n",
    "Create per-sample uncertainty (dropout at inference or predict variance) to decide when to fallback or ask for human in the loop.\n",
    "\n",
    "If stem is occluded in some side cases, add a mask/flag to skip or mark low confidence.\n",
    "\n",
    "6) Debug checklist (if predictions look off)\n",
    "\n",
    "Are label px,py correctly normalized relative to crop (0..1)?\n",
    "\n",
    "Are you using exactly the same normalization/resize pipeline during inference as training? (common bug)\n",
    "\n",
    "Are image channels order correct (RGB vs BGR)?\n",
    "\n",
    "Are you accidentally using full-image bbox normalization when the model expects crop-normalized?\n",
    "\n",
    "Visualize a random set of GT vs Pred on crops to spot biases (systematically off-center, always low, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fa327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59230819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3e712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc10649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov11_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
